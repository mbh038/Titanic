combi$Fare<-log(combi$Fare)
hist(combi$Fare)
summary(combi$Fare)
dataTrain <- combi[1:891,]
dataTest <- combi[892:1309,]
table(dataTrain$Cabin)
table(dataTest$Cabin)
library(caTools)
set.seed(1000)
spl = sample.split(dataTrain$Survived, SplitRatio = 0.8)
train = subset(dataTrain, spl==TRUE)
test = subset(dataTrain, spl==FALSE)
table(train$Cabin)
table(test$Cabin)
#   sex="female"
#   # train on one gender at a time
#   train<-subset(train,Sex==sex)
#   #train$Sex<-factor(train$Sex)
#   test<-subset(test,Sex==sex)
#   #test$Sex<-factor(test$Sex)
#   dataTest<-subset(dataTest,Sex==sex)
#   #dataTest$Sex<-factor(dataTest$Sex)
#train$Survived<-as.factor(train$Survived)
#test$Survived<-as.factor(test$Survived)
#   train$Cabin<-as.factor(train$Cabin)
#   test$Cabin<-as.factor(test$Cabin)
#   train$Cabin<-factor(train$Cabin)
#   test$Cabin<-factor(test$Cabin)
#   levels(test$Cabin) <- levels(train$Cabin)
## Models
model<-function(x){
md=list(8)
md[[1]]<-c(as.formula("as.factor(Survived)~Sex+Pclass"),2)
md[[2]]<-c(as.formula("as.factor(Survived)~Sex+Pclass+Fare"),3)
md[[3]]<-c(as.formula("as.factor(Survived)~Sex+Pclass+Fare+FamilySize"),4)
md[[4]]<-c(as.formula("as.factor(Survived)~Sex+Pclass+Fare+Age+FamilySize"),5) # Age na rows removed
md[[5]]<-c(as.formula("as.factor(Survived)~Sex+Pclass+Fare+Age+Title +FamilyID2+FamilySize +Cabin"),8)
md[[6]]<-c(as.formula("as.factor(Survived) ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked + Title + FamilySize + FamilyID2"),10)
md[[7]]<-c(as.formula("as.factor(Survived) ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked + Title + FamilySize + FamilyID2+Cabin"),11)
md[[8]]<-c(as.formula("as.factor(Survived) ~ Pclass + Sex + Age +Fare + Embarked + Title + FamilySize + FamilyID2+Cabin"),9)
md[[9]]<-c(as.formula("Survived ~ Pclass + Sex + Age +Fare + Embarked + Title + FamilySize + FamilyID2+Cabin"),9)
md[[x]]
}
require(gbm)
set.seed(567)
train=sample(1:nrow(dataTrain),600)
boost.titanic=gbm(model(9)[[1]],data=dataTrain[train,],distribution="bernoulli",n.trees=2000,shrinkage=0.01,interaction.depth=4)
thresholds=seq(0.1,1,0.1)
set.seed(567)
test.acc=double(10)
for (i in 1:10){
boost.pred=predict(boost.titanic,newdata=dataTrain[-train,],n.trees=2000,type="response")
boost.pred=ifelse(boost.pred>thresholds[i],1,0)
test.acc[i]=sum(boost.pred==dataTrain[-train,]$Survived)/nrow(dataTrain[-train,])
}
plot(thresholds,test.acc,pch=19,ylab="Accuracy", xlab="Threshold",main="Boosting Test Accuracy")
# fi nd best threshold
require(gbm)
set.seed(567)
train=sample(1:nrow(dataTrain),600)
boost.titanic=gbm(model(9)[[1]],data=dataTrain[train,],distribution="bernoulli",n.trees=200,shrinkage=0.01,interaction.depth=4)
thresholds=seq(0.1,1,0.1)
set.seed(567)
test.acc=double(10)
for (i in 1:10){
boost.pred=predict(boost.titanic,newdata=dataTrain[-train,],n.trees=2000,type="response")
boost.pred=ifelse(boost.pred>thresholds[i],1,0)
test.acc[i]=sum(boost.pred==dataTrain[-train,]$Survived)/nrow(dataTrain[-train,])
}
plot(thresholds,test.acc,pch=19,ylab="Accuracy", xlab="Threshold",main="Boosting Test Accuracy")
# fi nd best threshold
require(gbm)
set.seed(567)
train=sample(1:nrow(dataTrain),600)
boost.titanic=gbm(model(9)[[1]],data=dataTrain[train,],distribution="bernoulli",n.trees=2000,shrinkage=0.01,interaction.depth=4)
thresholds=seq(0.1,1,0.1)
set.seed(567)
test.acc=double(10)
for (i in 1:10){
boost.pred=predict(boost.titanic,newdata=dataTrain[-train,],n.trees=2000,type="response")
boost.pred=ifelse(boost.pred>thresholds[i],1,0)
test.acc[i]=sum(boost.pred==dataTrain[-train,]$Survived)/nrow(dataTrain[-train,])
}
plot(thresholds,test.acc,pch=19,ylab="Accuracy", xlab="Threshold",main="Boosting Test Accuracy")
require(gbm)
train=sample(1:nrow(dataTrain),600)
boost.titanic=gbm(model(9)[[1]],data=dataTrain[train,],distribution="bernoulli",n.trees=10000,shrinkage=0.01,interaction.depth=4)
summary(boost.titanic)
plot(boost.titanic,i="Fare")
plot(boost.titanic,i="Age")
# Lets make a prediction on the test set. With boosting, the number of
# trees is a tuning parameter, and if we have too many we can overfit.
# So we should use cross-validation to select the number of trees.
# We will leave this as an exercise. Instead, we will compute the test
# error as a function of the number of trees, and make a plot.
set.seed(567)
n.trees=seq(from=100,to=10000,by=100)
predmat=predict(boost.titanic,newdata=dataTrain[-train,],n.trees=n.trees,type="response")
dim(predmat)
predmat=ifelse(predmat>0.5,1,0)
#berr=with(dataTrain[-train,],apply( (predmat-medv)^2,2,mean))
test.acc=double(ncol(predmat))
for (i in 1:ncol(predmat)){
test.acc[i]=sum(predmat[,i]==dataTrain[-train,]$Survived)/nrow(dataTrain[-train,])
}
plot(n.trees,test.acc,pch=19,ylab="Accuracy", xlab="# Trees",main="Boosting Test Error")
#abline(h=min(test.err),col="red")
set.seed(567)
n.trees=seq(from=100,to=10000,by=100)
boost.titanic=gbm(model(9)[[1]],data=dataTrain[train,],distribution="bernoulli",n.trees=n.trees,shrinkage=0.01,interaction.depth=4)
predmat=predict(boost.titanic,newdata=dataTrain[-train,],n.trees=n.trees,type="response")
dim(predmat)
predmat=ifelse(predmat>0.5,1,0)
#berr=with(dataTrain[-train,],apply( (predmat-medv)^2,2,mean))
test.acc=double(ncol(predmat))
for (i in 1:ncol(predmat)){
test.acc[i]=sum(predmat[,i]==dataTrain[-train,]$Survived)/nrow(dataTrain[-train,])
}
plot(n.trees,test.acc,pch=19,ylab="Accuracy", xlab="# Trees",main="Boosting Test Error")
#abline(h=min(test.err),col="red")
set.seed(567)
n.trees=seq(from=100,to=10000,by=100)
boost.titanic=gbm(model(9)[[1]],data=dataTrain[train,],distribution="bernoulli",n.trees=n.trees,shrinkage=0.01,interaction.depth=4)
length(n.trees)
?sapply
set.seed(567)
n.trees=seq(from=100,to=10000,by=100)
for (i in 1:100){
boost.titanic[i]=gbm(model(9)[[1]],data=dataTrain[train,],distribution="bernoulli",n.trees=n.trees[i],shrinkage=0.01,interaction.depth=4)
predmat[i]=predict(boost.titanic[i],newdata=dataTrain[-train,],n.trees=n.trees[i],type="response")
}
set.seed(567)
n.trees=seq(from=100,to=10000,by=100)
for (i in 1:100){
boost.titanic=gbm(model(9)[[1]],data=dataTrain[train,],distribution="bernoulli",n.trees=n.trees[i],shrinkage=0.01,interaction.depth=4)
predmat[i]=predict(boost.titanic,newdata=dataTrain[-train,],n.trees=n.trees[i],type="response")
}
set.seed(567)
n.trees=seq(from=100,to=10000,by=100)
predmat=list()
for (i in 1:10){
boost.titanic=gbm(model(9)[[1]],data=dataTrain[train,],distribution="bernoulli",n.trees=n.trees[i],shrinkage=0.01,interaction.depth=4)
predmat[[i]]=predict(boost.titanic,newdata=dataTrain[-train,],n.trees=n.trees[i],type="response")
}
str(predmat)
predmat[[1]]
dim(predmat)
predmat=ifelse(predmat>0.5,1,0)
set.seed(567)
n.trees=seq(from=100,to=10000,by=100)
predmat=list()
for (i in 1:10){
boost.titanic=gbm(model(9)[[1]],data=dataTrain[train,],distribution="bernoulli",n.trees=n.trees[i],shrinkage=0.01,interaction.depth=4)
predmat[[i]]=predict(boost.titanic,newdata=dataTrain[-train,],n.trees=n.trees[i],type="response")
predmat[[i]]=ifelse(predmat[[i]]>0.5,1,0)
}
str(predmat)
summary(predmat)
sum(predmat[[1]])
sum(predmat[[2]])
sum(predmat[[3]])
sum(predmat[[4]])
sum(predmat[[5]])
sum(predmat[[6]])
dim(predmat)
ncol(predmat)
str(predmat)
length(predmat)
predmat[[1]][,1:10]
predmat[[1]][1:10]
length(predmat[[1]])
setwd("H:/Rspace/Titanic")
set.seed(1)
rm(list=ls())
dataTrain<-read.csv("./data/train.csv")
dataTest<-read.csv("./data/test.csv")
#str(dataTrain)
#summary(dataTrain)
dataTest$Survived <- NA
combi <- rbind(dataTrain, dataTest)
summary(combi)
summary(combi$Embarked)
combi$Embarked[which(combi$Embarked == '')]<-"S"
summary(combi$Embarked)
combi$Embarked <- factor(combi$Embarked)
summary(combi$Embarked)
# make title variable
combi$Name <- as.character(combi$Name)
#strsplit(combi$Name[1], split='[,.]')[[1]][2]
combi$Title <- sapply(combi$Name, FUN=function(x) {strsplit(x, split='[,.]')[[1]][2]})
combi$Title <- sub(' ', '', combi$Title)
table(combi$Title)
combi$Title[combi$Title %in% c('Mme', 'Mlle','Ms')] <- 'Mlle'
combi$Title[combi$Title %in% c('Capt', 'Don', 'Major', 'Sir','Jonkheer')] <- 'Sir'
combi$Title[combi$Title %in% c('Dona', 'Lady', 'the Countess')] <- 'Lady'
table(combi$Title)
combi$Title<-factor(combi$Title)
# make family size variable
combi$FamilySize <- combi$SibSp + combi$Parch + 1
# make familyID variable
combi$Surname <- sapply(combi$Name, FUN=function(x) {strsplit(x, split='[,.]')[[1]][1]})
combi$FamilyID <- paste(as.character(combi$FamilySize), combi$Surname, sep="")
combi$FamilyID[combi$FamilySize <= 2] <- 'Small'
famIDs <- data.frame(table(combi$FamilyID))
famIDs <- famIDs[famIDs$Freq <= 2,]
combi$FamilyID[combi$FamilyID %in% famIDs$Var1] <- 'Small'
combi$FamilyID <- factor(combi$FamilyID)
table(combi$FamilyID)
# reduce levels
combi$FamilyID2 <- combi$FamilyID
combi$FamilyID2 <- as.character(combi$FamilyID2)
combi$FamilyID2[combi$FamilySize <= 3] <- 'Small'
combi$FamilyID2 <- factor(combi$FamilyID2)
# make cabin variable
cabins<-function(x){
cabinLevels<-c("A","B","C","D","E","F","G","H","T")
x<-as.character(x)
x<-substr(x, 1, 1)
x[x==""] <- "H"
newX<-as.factor(x)
levels(newX)<-cabinLevels
invisible(newX)
}
combi$Cabin<-cabins(combi$Cabin)
#normalise age variable
combi$Age<-(combi$Age-mean(combi$Age,na.rm=TRUE))/sd(combi$Age,na.rm=TRUE)
library(rpart)
# impute missing age values
Agefit <- rpart(Age ~ Pclass + Sex + SibSp + Parch + Fare + Embarked + Title + FamilySize,
data=combi[!is.na(combi$Age),], method="anova")
combi$Age[is.na(combi$Age)] <- predict(Agefit, combi[is.na(combi$Age),])
summary(combi$Age)
# impute missing fare value
Farefit <- rpart(Fare ~ Pclass + Sex + SibSp + Parch + Embarked + Title + FamilySize,
data=combi[!is.na(combi$Fare),], method="anova")
combi$Fare[is.na(combi$Fare)] <- predict(Farefit, combi[is.na(combi$Fare),])
summary(combi$Fare)
Farefit <- rpart(Fare ~ Pclass + Sex + SibSp + Parch + Embarked + Title + FamilySize,
data=combi[combi$Fare!=0,], method="anova")
combi$Fare[combi$Fare==0] <- predict(Farefit, combi[combi$Fare==0,])
summary(combi$Fare)
#transform fare values
combi$Fare<-log(combi$Fare)
hist(combi$Fare)
summary(combi$Fare)
dataTrain <- combi[1:891,]
dataTest <- combi[892:1309,]
table(dataTrain$Cabin)
table(dataTest$Cabin)
library(caTools)
set.seed(1000)
spl = sample.split(dataTrain$Survived, SplitRatio = 0.8)
train = subset(dataTrain, spl==TRUE)
test = subset(dataTrain, spl==FALSE)
table(train$Cabin)
table(test$Cabin)
#   sex="female"
#   # train on one gender at a time
#   train<-subset(train,Sex==sex)
#   #train$Sex<-factor(train$Sex)
#   test<-subset(test,Sex==sex)
#   #test$Sex<-factor(test$Sex)
#   dataTest<-subset(dataTest,Sex==sex)
#   #dataTest$Sex<-factor(dataTest$Sex)
#train$Survived<-as.factor(train$Survived)
#test$Survived<-as.factor(test$Survived)
#   train$Cabin<-as.factor(train$Cabin)
#   test$Cabin<-as.factor(test$Cabin)
#   train$Cabin<-factor(train$Cabin)
#   test$Cabin<-factor(test$Cabin)
#   levels(test$Cabin) <- levels(train$Cabin)
## Models
model<-function(x){
md=list(8)
md[[1]]<-c(as.formula("as.factor(Survived)~Sex+Pclass"),2)
md[[2]]<-c(as.formula("as.factor(Survived)~Sex+Pclass+Fare"),3)
md[[3]]<-c(as.formula("as.factor(Survived)~Sex+Pclass+Fare+FamilySize"),4)
md[[4]]<-c(as.formula("as.factor(Survived)~Sex+Pclass+Fare+Age+FamilySize"),5) # Age na rows removed
md[[5]]<-c(as.formula("as.factor(Survived)~Sex+Pclass+Fare+Age+Title +FamilyID2+FamilySize +Cabin"),8)
md[[6]]<-c(as.formula("as.factor(Survived) ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked + Title + FamilySize + FamilyID2"),10)
md[[7]]<-c(as.formula("as.factor(Survived) ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked + Title + FamilySize + FamilyID2+Cabin"),11)
md[[8]]<-c(as.formula("as.factor(Survived) ~ Pclass + Sex + Age +Fare + Embarked + Title + FamilySize + FamilyID2+Cabin"),9)
md[[9]]<-c(as.formula("Survived ~ Pclass + Sex + Age +Fare + Embarked + Title + FamilySize + FamilyID2+Cabin"),9)
md[[x]]
}
require(gbm)
train=sample(1:nrow(dataTrain),600)
boost.titanic=gbm(model(9)[[1]],data=dataTrain[train,],distribution="bernoulli",n.trees=10000,shrinkage=0.01,interaction.depth=4)
summary(boost.titanic)
plot(boost.titanic,i="Fare")
plot(boost.titanic,i="Age")
set.seed(567)
n.trees=seq(from=100,to=10000,by=100)
predmat=predict(boost.titanic,newdata=dataTrain[-train,],n.trees=n.trees,type="response")
dim(predmat)
predmat=ifelse(predmat>0.5,1,0)
#berr=with(dataTrain[-train,],apply( (predmat-medv)^2,2,mean))
test.acc=double(ncol(predmat))
for (i in 1:ncol(predmat)){
test.acc[i]=sum(predmat[,i]==dataTrain[-train,]$Survived)/nrow(dataTrain[-train,])
}
plot(n.trees,test.acc,pch=19,ylab="Accuracy", xlab="# Trees",main="Boosting Test Error")
#abline(h=min(test.err),col="red")
require(gbm)
train=sample(1:nrow(dataTrain),600)
boost.titanic=gbm(model(9)[[1]],data=dataTrain[train,],distribution="bernoulli",n.trees=2000,shrinkage=0.01,interaction.depth=4)
summary(boost.titanic)
plot(boost.titanic,i="Fare")
plot(boost.titanic,i="Age")
set.seed(567)
n.trees=seq(from=100,to=10000,by=100)
predmat=predict(boost.titanic,newdata=dataTrain[-train,],n.trees=n.trees,type="response")
dim(predmat)
predmat=ifelse(predmat>0.5,1,0)
#berr=with(dataTrain[-train,],apply( (predmat-medv)^2,2,mean))
test.acc=double(ncol(predmat))
for (i in 1:ncol(predmat)){
test.acc[i]=sum(predmat[,i]==dataTrain[-train,]$Survived)/nrow(dataTrain[-train,])
}
plot(n.trees,test.acc,pch=19,ylab="Accuracy", xlab="# Trees",main="Boosting Test Error")
#abline(h=min(test.err),col="red")
set.seed(567)
n.trees=seq(from=100,to=2000,by=100)
predmat=predict(boost.titanic,newdata=dataTrain[-train,],n.trees=n.trees,type="response")
dim(predmat)
predmat=ifelse(predmat>0.5,1,0)
#berr=with(dataTrain[-train,],apply( (predmat-medv)^2,2,mean))
test.acc=double(ncol(predmat))
for (i in 1:ncol(predmat)){
test.acc[i]=sum(predmat[,i]==dataTrain[-train,]$Survived)/nrow(dataTrain[-train,])
}
plot(n.trees,test.acc,pch=19,ylab="Accuracy", xlab="# Trees",main="Boosting Test Error")
#abline(h=min(test.err),col="red")
require(gbm)
train=sample(1:nrow(dataTrain),600)
boost.titanic=gbm(model(9)[[1]],data=dataTrain[train,],distribution="bernoulli",n.trees=2000,shrinkage=0.01,interaction.depth=4)
summary(boost.titanic)
plot(boost.titanic,i="Fare")
plot(boost.titanic,i="Age")
set.seed(567)
n.trees=seq(from=100,to=2000,by=100)
predmat=predict(boost.titanic,newdata=dataTrain[-train,],n.trees=n.trees,type="response")
dim(predmat)
predmat=ifelse(predmat>0.5,1,0)
#berr=with(dataTrain[-train,],apply( (predmat-medv)^2,2,mean))
test.acc=double(ncol(predmat))
for (i in 1:ncol(predmat)){
test.acc[i]=sum(predmat[,i]==dataTrain[-train,]$Survived)/nrow(dataTrain[-train,])
}
plot(n.trees,test.acc,pch=19,ylab="Accuracy", xlab="# Trees",main="Boosting Test Error")
#ab
require(gbm)
train=sample(1:nrow(dataTrain),600)
boost.titanic=gbm(model(9)[[1]],data=dataTrain[train,],distribution="bernoulli",n.trees=2000,shrinkage=0.01,interaction.depth=4)
summary(boost.titanic)
plot(boost.titanic,i="Fare")
plot(boost.titanic,i="Age")
# Lets make a prediction on the test set. With boosting, the number of
# trees is a tuning parameter, and if we have too many we can overfit.
# So we should use cross-validation to select the number of trees.
# We will leave this as an exercise. Instead, we will compute the test
# error as a function of the number of trees, and make a plot.
set.seed(567)
n.trees=seq(from=100,to=2000,by=20)
predmat=predict(boost.titanic,newdata=dataTrain[-train,],n.trees=n.trees,type="response")
dim(predmat)
predmat=ifelse(predmat>0.5,1,0)
#berr=with(dataTrain[-train,],apply( (predmat-medv)^2,2,mean))
test.acc=double(ncol(predmat))
for (i in 1:ncol(predmat)){
test.acc[i]=sum(predmat[,i]==dataTrain[-train,]$Survived)/nrow(dataTrain[-train,])
}
plot(n.trees,test.acc,pch=19,ylab="Accuracy", xlab="# Trees",main="Boosting Test Error")
#abline(h=min(test.err),col="red")
require(gbm)
train=sample(1:nrow(dataTrain),600)
boost.titanic=gbm(model(9)[[1]],data=dataTrain[train,],distribution="bernoulli",n.trees=2000,shrinkage=0.01,interaction.depth=4)
summary(boost.titanic)
plot(boost.titanic,i="Fare")
plot(boost.titanic,i="Age")
# Lets make a prediction on the test set. With boosting, the number of
# trees is a tuning parameter, and if we have too many we can overfit.
# So we should use cross-validation to select the number of trees.
# We will leave this as an exercise. Instead, we will compute the test
# error as a function of the number of trees, and make a plot.
set.seed(567)
n.trees=seq(from=100,to=2000,by=20)
predmat=predict(boost.titanic,newdata=dataTrain[-train,],n.trees=n.trees,type="response")
dim(predmat)
predmat=ifelse(predmat>0.5,1,0)
#berr=with(dataTrain[-train,],apply( (predmat-medv)^2,2,mean))
test.acc=double(ncol(predmat))
for (i in 1:ncol(predmat)){
test.acc[i]=sum(predmat[,i]==dataTrain[-train,]$Survived)/nrow(dataTrain[-train,])
}
plot(n.trees,test.acc,pch=19,ylab="Accuracy", xlab="# Trees",main="Boosting Test Error")
#abline(h=min(test.err),col="red")
require(gbm)
train=sample(1:nrow(dataTrain),600)
boost.titanic=gbm(model(9)[[1]],data=dataTrain[train,],distribution="bernoulli",n.trees=2000,shrinkage=0.01,interaction.depth=4)
summary(boost.titanic)
plot(boost.titanic,i="Fare")
plot(boost.titanic,i="Age")
# Lets make a prediction on the test set. With boosting, the number of
# trees is a tuning parameter, and if we have too many we can overfit.
# So we should use cross-validation to select the number of trees.
# We will leave this as an exercise. Instead, we will compute the test
# error as a function of the number of trees, and make a plot.
#set.seed(567)
n.trees=seq(from=100,to=2000,by=20)
predmat=predict(boost.titanic,newdata=dataTrain[-train,],n.trees=n.trees,type="response")
dim(predmat)
predmat=ifelse(predmat>0.5,1,0)
#berr=with(dataTrain[-train,],apply( (predmat-medv)^2,2,mean))
test.acc=double(ncol(predmat))
for (i in 1:ncol(predmat)){
test.acc[i]=sum(predmat[,i]==dataTrain[-train,]$Survived)/nrow(dataTrain[-train,])
}
plot(n.trees,test.acc,pch=19,ylab="Accuracy", xlab="# Trees",main="Boosting Test Error")
#abline(h=min(test.err),col="red")
require(gbm)
train=sample(1:nrow(dataTrain),600)
boost.titanic=gbm(model(9)[[1]],data=dataTrain[train,],distribution="bernoulli",n.trees=2000,shrinkage=0.01,interaction.depth=4)
summary(boost.titanic)
plot(boost.titanic,i="Fare")
plot(boost.titanic,i="Age")
# Lets make a prediction on the test set. With boosting, the number of
# trees is a tuning parameter, and if we have too many we can overfit.
# So we should use cross-validation to select the number of trees.
# We will leave this as an exercise. Instead, we will compute the test
# error as a function of the number of trees, and make a plot.
#set.seed(567)
n.trees=seq(from=100,to=2000,by=20)
predmat=predict(boost.titanic,newdata=dataTrain[-train,],n.trees=n.trees,type="response")
dim(predmat)
predmat=ifelse(predmat>0.5,1,0)
#berr=with(dataTrain[-train,],apply( (predmat-medv)^2,2,mean))
test.acc=double(ncol(predmat))
for (i in 1:ncol(predmat)){
test.acc[i]=sum(predmat[,i]==dataTrain[-train,]$Survived)/nrow(dataTrain[-train,])
}
plot(n.trees,test.acc,pch=19,ylab="Accuracy", xlab="# Trees",main="Boosting Test Error")
#abline(h=min(test.err),col="red")
require(gbm)
train=sample(1:nrow(dataTrain),600)
boost.titanic=gbm(model(9)[[1]],data=dataTrain[train,],distribution="bernoulli",n.trees=2000,shrinkage=0.01,interaction.depth=4)
summary(boost.titanic)
plot(boost.titanic,i="Fare")
plot(boost.titanic,i="Age")
# Lets make a prediction on the test set. With boosting, the number of
# trees is a tuning parameter, and if we have too many we can overfit.
# So we should use cross-validation to select the number of trees.
# We will leave this as an exercise. Instead, we will compute the test
# error as a function of the number of trees, and make a plot.
#set.seed(567)
n.trees=seq(from=100,to=2000,by=20)
predmat=predict(boost.titanic,newdata=dataTrain[-train,],n.trees=n.trees,type="response")
dim(predmat)
predmat=ifelse(predmat>0.5,1,0)
#berr=with(dataTrain[-train,],apply( (predmat-medv)^2,2,mean))
test.acc=double(ncol(predmat))
for (i in 1:ncol(predmat)){
test.acc[i]=sum(predmat[,i]==dataTrain[-train,]$Survived)/nrow(dataTrain[-train,])
}
plot(n.trees,test.acc,pch=19,ylab="Accuracy", xlab="# Trees",main="Boosting Test Error")
#abline(h=min(test.err),col="red")
require(gbm)
train=sample(1:nrow(dataTrain),600)
boost.titanic=gbm(model(9)[[1]],data=dataTrain[train,],distribution="bernoulli",n.trees=2000,shrinkage=0.01,interaction.depth=4)
summary(boost.titanic)
plot(boost.titanic,i="Fare")
plot(boost.titanic,i="Age")
# Lets make a prediction on the test set. With boosting, the number of
# trees is a tuning parameter, and if we have too many we can overfit.
# So we should use cross-validation to select the number of trees.
# We will leave this as an exercise. Instead, we will compute the test
# error as a function of the number of trees, and make a plot.
#set.seed(567)
n.trees=seq(from=100,to=2000,by=20)
predmat=predict(boost.titanic,newdata=dataTrain[-train,],n.trees=n.trees,type="response")
dim(predmat)
predmat=ifelse(predmat>0.5,1,0)
#berr=with(dataTrain[-train,],apply( (predmat-medv)^2,2,mean))
test.acc=double(ncol(predmat))
for (i in 1:ncol(predmat)){
test.acc[i]=sum(predmat[,i]==dataTrain[-train,]$Survived)/nrow(dataTrain[-train,])
}
plot(n.trees,test.acc,pch=19,ylab="Accuracy", xlab="# Trees",main="Boosting Test Error")
#abline(h=min(test.err),col="red")
require(gbm)
train=sample(1:nrow(dataTrain),600)
boost.titanic=gbm(model(9)[[1]],data=dataTrain[train,],distribution="bernoulli",n.trees=2000,shrinkage=0.01,interaction.depth=4)
summary(boost.titanic)
plot(boost.titanic,i="Fare")
plot(boost.titanic,i="Age")
# Lets make a prediction on the test set. With boosting, the number of
# trees is a tuning parameter, and if we have too many we can overfit.
# So we should use cross-validation to select the number of trees.
# We will leave this as an exercise. Instead, we will compute the test
# error as a function of the number of trees, and make a plot.
#set.seed(567)
n.trees=seq(from=100,to=2000,by=20)
predmat=predict(boost.titanic,newdata=dataTrain[-train,],n.trees=n.trees,type="response")
dim(predmat)
predmat=ifelse(predmat>0.5,1,0)
#berr=with(dataTrain[-train,],apply( (predmat-medv)^2,2,mean))
test.acc=double(ncol(predmat))
for (i in 1:ncol(predmat)){
test.acc[i]=sum(predmat[,i]==dataTrain[-train,]$Survived)/nrow(dataTrain[-train,])
}
plot(n.trees,test.acc,pch=19,ylab="Accuracy", xlab="# Trees",main="Boosting Test Error")
#abline(h=min(test.err),col="red")
